# 9장_웹 크롤러 설계

## 크롤러 사용용도

### search engine indexing

- 웹페이지를 모아 검색 엔진을 위한 local index를 만듬
- ex) Googlebot : 구글 검색 엔진이 사용하는 웹 크롤러

### web archiving

- 나중에 사용할 목적으로 장기 보관하기 위해 웹에서 정보를 모으는 절차
- ex) 국립 도서관에서 크롤러를 돌려 웹 사이트를 아카이빙 함

### web mining

- ex) 금융 기업에서 크롤러를 활용하여 주주총회 자료를 다운받아 기업의 핵심 사업 방향을 알아내기도 함

### web monitoring

- ex) 인터넷에서 저작권이나 상표권이 침해되는 사례 모니터링

# 크롤러 설계

## 1. 문제 이해 및 설계 범위 확정

- 웹 크롤러의 기본 알고리즘
    1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지 다운로드
    2. 다운받은 웹 페이지에서 URL 추출
    3. 추출된 URL들을 다운로드할 URL 목록에 추가 & 1번부터 다시 반복
- 요구사항을 알아내고, 모호한 부분을 제거하기 위한 질문하기
    - 크롤러의 주된 용도는?
    - 매달 얼마나 많은 웹 페이지를 수집해야 하는가?
    - 새로 만들어지거나 수정된 웹 페이지를 고려해야 하는가?
    - 수집한 웹 페이지는 저장해야 하나?
    - 중복된 콘텐츠는 어떻게 처리하나?
- 개략적인 규모 추정하기
    - QPS
    - 저장용량
- 좋은 웹 크롤러의 조건
    - 규모 확장성
        - parallelism을 활용하면 효과적으로 웹 크롤링을 할 수 있음
    - 안정성
        - 잘못 작성된 HTML, 악성 코드가 붙은 링크, 아무 반응이 없는 서버 등 비정상적인 입력에 잘 대응할 수 있어야 함
    - 예절(politeness)
        - 크롤러는 수집 대상 웹사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안됌
    - 확장성
        - 새로운 형태의 콘텐츠를 지원하기 쉬워야 함

## 2. 개략적 설계안 제시 및 동의 구하기

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c572571b-8a19-4c28-929f-c23789a58601/f164804e-692c-4d4c-954c-fcced665ac5d/Untitled.png)

### 구성 요소

- 시작 URL 집합
    - 전체 URL 공간을 작은 부분 집합으로 나누는 전략 주로 사용
- 미수집 URL 저장소
    - 크롤링 상태 중 다운로드할 URL을 저장 관리하는 컴포넌트 (FIFO queue)
        - 다운로드할 URL
        - 다운로드 된 URL
    - 상세 구현
        - 우선순위
            - 순위 결정 장치 : URL 우선순위를 정하는 컴포넌트
        - politeness
            - 동일 웹사이트에 대해서는 한번에 한 페이지만 요청
            - 같은 웹 사이트의 페이지를 다운받는 task는 시간차를 두고 실행
            - 같은 호스트에 속한 URL은 같은 queue에 보관
        - 신선도
            - 데이터의 신선함 유지를 위해 이미 다운로드한 페이지도 주기적으로 재수집
            - 웹 페이지의 변경 이력 활용
            - 우선 순위 활용 → 중요한 페이지는 더 자주 재수집
        - 지속성 저장장치
            - 대부분의 URL은 디스크에 두고,
            - IO비용을 줄이기 위해 메모리 버퍼에 큐를 둠
            - 버퍼에 있는 데이터는 주기적으로 디스크에 기록
- HTML 다운로더
    - 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
    - 상세 구현
        - Robots.txt
            - 웹사이트가  크롤러와 소통하는 표준적 방법
            - 크롤러가 수집해도 되는 페이지 목록이 적혀있음
        - 성능 최적화 방법
            - 분산 크롤링
                - 크롤링 작업을 여러 서버에 분산하는 방법
            - 도메인 이름 변환 결과 캐시
                - 크롤러 스레드 하나가 DNS 요청을 보내고 결과를 받는 작업을 하고 있으면, 다른 스레드의 DNS 요청은 전부 block됌
                - DNS 조회 결과로 얻어진 도메인 이름 & IP 주소의 관계를 캐시에 보관
                - cron job을 돌려 주기적으로 갱신하도록 해놓으면 성능을 높일 수 있음
            - 지역성
                - 크롤링 작업을 수행하는 서버를 지역별로 분산 → 페이지 다운로드 시간 줄어듬
            - 짧은 타임아웃
                - 정해둔 시간 동안 서버가 응답하지 않으면 크롤러는 해당 페이지 다운로드를 중단하고 다음 페이지로 넘어감
        - 시스템 안정성을 향상시키기 위한 접근법
            - 안정 해시(consistent hashing)
                - 다운로더 서버들에 부하를 분산할 때 적용
                - 다운로더 서버를 쉽게 추가/삭제 할 수 있음
            - 크롤링 상태 및 수집 데이터 저장
                - 장애 발생 시 쉽게 복구 가능
            - 예외 처리
            - 데이터 검증
                - 시스템 오류 방지
        - 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 하는 ‘확장성’
            - ex) URL 추출기 / PNG 다운로더 / 웹 모니터 ← 모듈 확장
- 도메인 이름 변환기(DNS resolver)
    - HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP주소를 알아냄
    - DNS 요청을 보내고 결과를 받는 작업의 동기적 특성 → 크롤러 성능의 병목 야기
- 콘텐츠 parser
    - 크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이 느려짐 → 독립된 컴포넌트로 만듬
- 중복 콘텐츠 인가?
    - 웹 페이지의 해시 값 비교
    - 자료구조 도입 → 데이터 중복을 줄이고, 데이터 처리에 소요되는 시간 줄임
- 콘텐츠 저장소
    - HTML 문서를 보관하는 시스템
    - 대부분의 콘텐츠는 디스크에 저장 (데이터 양이 많아서)
    - 인기 있는 콘텐츠는 메모리에 저장 (빠른 접근 가능)
- URL 추출기
    - HTML 페이지를 파싱하여 링크들을 골라내는 역할
- URL 필터
    - 특정 콘텐츠 타입, 파일 확장자를 갖는 URL 등을 크롤링 대상에서 배제하는 역할
- 이미 방문한 URL?
    - URL 저장소에 보관된 URL을 추적할 수 있도록 하는 자료 구조 사용 (**블룸 필터, 해시 테이블**)
- URL 저장소
    - 이미 방문한 URL 보관

## 3. 상세 설계

### DFS vs BFS

- 페이지 : 노드, URL : edge
- 크롤링 프로세스 : 페이지와 URL로 이루어진 directed graph를 edge를 따라 탐색하는 과정
- DFS
    - 그래프 크기가 클 경우, 깊이가 얼마나 될지 가늠할 수 없음
- BFS : 주로 사용
    - 문제점
        - impolite 크롤러 : 한 페이지에서 나오는 링크는 대부분 같은 서버로 되돌아감 → 링크들을 병렬로 처리하게 되면, 위키피디아(예시) 서버는 수많은 요청으로 과부하에 걸리게 됌
        - URL 간에 우선순위를 두지 않음

### 문제 있는 콘텐츠 감지 및 회피 전략

- 중복 콘텐츠 탐지 방법
    - hash
    - check-sum
- 거미 덫 (크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지) 회피 방법
    - URL의 최대 길이 제한하기
    - 사람이 직접 덫을 확인하고, 덫이 있는 사이트를 크롤러 탐색 대상에서 제외하거나 URL 필터 목록에 걸어두기
    - 만능 해결책은 없음
- 데이터 노이즈
    - 광고, 스팸 URL 처럼 가치가 없는 콘텐츠는 크롤러 탐색 대상에서 제외
