# 키-값 저장소 설계
key-value store는 non-relational 데이터베이스로 key는 unique해야하며 value는 무엇이 오든지 상관없다. key-value store로 널리 알려진 것은 Amazon DynamoDB, memcached, Redis 등이 있다.

## 단일 서버 키-값 저장소
한 대 서버만 사용해서 key-value store를 설계하는 일은 쉽지만 데이터가 많아지게 되면 한 대의 서버만으로 운영하기란 쉽지 않다. 이때 분산 키-값 저장소를 만들 필요가 있다.

## 분산 키-값 저장소
분산 키-값 저장소는 분산 해시 테이블이라고도 불린다. key-value pair를 여러 서버에 분산시키기 때문이다. 그리고 분산 시스템을 설계할 때는 CAP를 잘 이해하고 있어야 한다.

### CAP 정리
CAP 정리는 데이터 일관성(consistency), 가용성(availability), 파티션 감내(partition tolerance)라는 세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다는 정리이다. (여기서 파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미한다. 즉, 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작하여야 한다는 것을 말한다.)

키-값 저장소는 앞서 제시한 세가지 요구사항 중 어느 두가지를 만족하냐에 따라 분류할 수 있다.
- CP 시스템: 일관성과 파티션 감내를 지원하는 key-value store, 가용성을 희생. 데이터의 불일치를 피하기 위해 쓰기 연산을 중단.
- AP 시스템: 가용성과 파티션 감내를 지원하는 key-value store, 일관성을 희생. 계속해서 제공하기 위해 계속 읽기 연산을 허용 and 쓰기 연산도 허용.
- CA 시스템: 가용성과 일관성를 지원하는 key-value store, 파티션 감내를 희생. 하지만 통신 네트워크 장애는 피할 수 없는 일로 여겨지므로 반드시 파티션 문제를 감내할 수 있도록 설계되어야 한다. 결국 CA 시스템은 존재하지 않는다.

### 시스템 컴포넌트
- 데이터 파티션
- 데이터 다중화
- 일관성
- 일관성 불일치 해소
- 장애 처리
- 시스템 아키텍쳐 다이어그램
- 쓰기 경로
- 읽기 경로

#### 데이터 파티션
대규모 애플리케이션의 경우 전체 데이터를 한 대 서버에 욱여넣는 것은 불가능 => 데이터를 작은 파티션들로 분할한 다음 여러 대 서버에 분산 저장해야한다.

데이터를 파티션 단위로 나눌 때 고려해야할 점
- 데이터를 여러 서버에 고르게 분산할 수 있는가?
- 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가?

이때 적합한 기술은 이전 장에서 학습한 안정 해시 기술로 이 기술을 이용하여 데이터를 파티션하면 좋은 점은 다음과 같다.
- 규모 확장 자동화
- 다양성

#### 데이터 다중화
높은 가용성과 안정성을 확보하기 위해서는 데이터를 여러 개의 서버에 비동기적으로 replication할 필요가 있다. 이때 해시 링을 이용해서 어떤 키를 해시 링위에 배치한 후 그 지점으로부터 시계 방향으로 링을 순회하면서 만나는 첫 N개의 서버에 데이터 사본을 보관하는 것이다. 이때 가상 노드를 사용한다면 같은 물리 서버에 중복 선택하지 않도록 해야한다.

#### 데이터 일관성
여러 노드에 다중화된 데이터는 적절히 동기화 되어야 하는데 이때 정족수 합의 (Quorum Consensus) 프로토콜을 사용하면 read/write 연산 모두에 일관성을 보장할 수 있다.
만약 N이 사본 개수, W가 쓰기 연산에 대한 정족수(쓰기 연산이 success 되려면 적어도 W개의 서버로부터 write가 성공했다는 응답을 받아야 한다.), R가 읽기 연산에 대한 정족수(읽기 연산이 success 되려면 적어도 R개의 서버로부터 read가 성공했다는 응답을 받아야 한다.), 판단을 위한 중재자가 존재한다고 해보자.
- R=1, W=N이면 빠른 읽기 연산에 최적화된 시스템
- W=1, R=N이면 빠른 쓰기 연산에 최적화된 시스템
- W+R>N이면 강한 일관성이 보장되는 시스템
- W+R<=N이면 강한 일관성이 보장되지 않는 시스템

요구되는 일관성 수준에 따라 위의 값들을 조정하면 된다.

#### 일관성 모델
일관성 모델은 데이터의 일관성의 수준을 결정하는데, 종류가 다양하다
- 강한 일관성: 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환
- 약한 일관성: 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 가능성 존재
- 최종 일관성: 데이터 항목에 새로운 업데이트가 없으면 궁극적으로 해당 항목에 대한 모든 접근들은 마지막으로 업데이트된 값을 반환하는 것을 비공식적으로 보장하는 고가용성을 달성

보통 DynamoDB나 카산드라 같은 저장소는 최종 일관성 모델을 택하고 있다. 최종 일관성 모델을 따를 경우 쓰기 연산이 병렬적으로 발생하면 시스템에 저장된 값의 일관성이 깨어질 수 있는데 이 문제는 클라이언트가 해결해야 한다. 클라이언트 측에서 데이터의 버전 정보를 활용해 일관성이 깨진 데이터를 읽지 않도록 해야한다.

#### 비 일관성 해소 기법: 데이터 버저닝
데이터의 사본 간 일관성이 깨지는 가능성을 해소하기 위해 버저닝과 벡터시계 기술이 필요하다. 버저닝은 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것을 의미하고 각 버전의 데이터는 변경 불가능하다. 벡터 시계는 두 버전 사이의 충돌을 해소하기 위해 나온 기술이며 [서버, 버전]의 순서쌍을 데이터에 매단 것이다.

하지만 벡터 시계를 사용해 충돌을 감지하고 해소하는 방법에는 두 가지 분명한 단점이 존재한다. 첫 번째는 충돌 감지 및 해소 로직이 클라이언트에 들어가야 하므로 클라이언트 구현이 복잡해진다는 점이고 두 번째는 [서버,버전]의 순서쌍 개수가 굉장히 빨리 늘어나서 길이에 임계치를 설정하고 임계치 이상으로 길이가 길어지면 오래된 순서쌍을 벡터 시계에서 제거하도록 해야 한다.

#### 장애 처리
장애는 불가피한 것으로 우선 장애 감지 기법으로 장애를 감지하고 장애 해소 전략들을 이용해서 해소해야 한다.

#### 장애 감지
모든 노드 사이에 멀티캐스팅 채널을 구축하여 서버 장애를 감지하는 방법이 가장 쉬운 방법이다. 하지만 서버가 많아지면 비효율적이 되고 이럴 때는 가십 프로토콜 (gossip protocol) 같은 분산형 장애 감지 솔루션을 채택하는 편이 보다 효율적이다. (원리는 각각의 노드가 멤버십 목록을 유지하고 멤버십 목록은 각 멤버id와 박동 카운터 쌍의 목록. 각 노드는 주기적으로 자신의 박동 카운터를 증가시키고 주변 노드에 전파. 이때 어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 장애 상태인 것으로 추정)

#### 일시적 장애 처리
느슨한 정족수 (sloppy quorum) 접근법을 이용해 읽기, 쓰기 연산을 대신할 서버를 링에서 고른다. 이때 장애 상태인 서버의 요청을 고른 다른 서버들이 처리하고 임시로 쓰기 연산을 처리한 서버는 그에 관한 단서(hint)를 남긴다. 이러한 방법을 hinted handoff 기법이라고 부른다. 그리고 복구된 이후에 데이터를 옮긴다.

#### 영구 장애 처리
영구 장애 같은 경우는 반-엔트로피(anti-entropy) 프로토콜을 구현하여 사본들을 동기화한다. 해당 프로토콜은 사본들을 비교하여 최신 버전으로 갱신하는 과정을 포함한다. 사본 간의 일관성이 망가진 상태를 감지하고 전송 데이터의 양을 줄이기 위해서는 Merkle 트리를 사용한다.
- 1단계: 키 공간을 버킷으로 나눈다.
- 2단계: 버킷에 포함된 각각의 키에 균등 분포 해시 함수를 적용하여 해시 값을 계산한다.
- 3단계: 버킷별로 해시값을 계산한 후, 해당 해시 값을 레이블로 갖는 노드를 만든다.
- 4단계: 자식 노드의 레이블로부터 새로운 해시 값을 계산하여, 이진 트리를 상향식으로 구성해 나간다.

서버 사이의 머클 트리의 비교는 루트 노드의 해시값을 비교하는 것으로 시작하여 해시 값이 일치한다면 두 서버는 같은 데이터를 갖는 것이다. 그 값이 다른 경우에는 왼쪽 자식 노드의 해시 값과 비교, 오른쪽 자식 노드의 해시 값과 비교하면서 점차 아래쪽으로 탐색해 나가면서 다른 데이터를 갖는 버킷을 찾아서 그 버킷들만 동기화한다.

#### 시스템 아키텍쳐 다이어그램
- 클라이언트는 get과 put으로 통신한다.
- 중재자는 클라이언트에게 key-value store에 대한 proxy 역할을 하는 노드다
- 노드는 안정 해시의 해시 링 위에 분포한다.
- 노드를 자동으로 추가 또는 삭제할 수 있도록 시스템은 완전히 분산된다
- 데이터는 여러 노드에 다중화된다
- 모든 노드가 같은 책임을 지므로 SPOF는 존재하지 않는다

#### 쓰기 경로
1. 쓰기 요청이 커밋 로그 파일에 기록된다
2. 데이터가 메모리 캐시에 기록된다
3. 메모리 캐시가 가득차거나 임계치에 도달하면 데이터는 디스크에 있는 SSTable에 기록된다(Sorted-String Table)

#### 읽기 경로
1. 데이터가 메모리 있는지 검사한다. 없으면 2로 이동
2. 데이터가 메모리에 없으므로 Bloom filter를 검사한다
3. Bloom filter를 통해 어떤 SSTable에 키가 보관되어 있는 지 알아낸다
4. SSTable에서 데이터를 가져온다
5. 해당 데이터를 클라이언트에게 반환한다